#!/usr/bin/env python
"""
xbrain is a tool for relating non-neural scores and MRI data features.

Usage:
    xbrain [options] <database>

Arguments:
    <database>      a .csv file containing MRI data (X) and non-neural data (y)

Options:
    --timeseries    A comma-seperated list of timeseries .csv (2D) predictors.
    --connectivity  A comma-seperated list of timeseries .csv (2D) predictors.
    --statmaps      A comma-seperated list of statmap .nii.gz (3D) predictors.
    --roi_mask      A ROI .nii.gz file with the same dimensions as all input statmaps and the same number of non-zero ROIs as there are unique timeseries.
    --predict       A comma-seperated list of column names in database to predict.
    --k             Number of folds done for outer loop cross-validation [default: 10]
    --n_features    Number of tagret features per sample. If the number of raw features is larger than this number, uses PCA to reduce the data to this number.
    --target-cutoff Percentile cutoff (0 < p < 0.5) to seperate the low and high y score group [default: 0.5]
    --target-group  Draw the template from this group [default: 1]
    --two-template  If defined, features are the difference between target-group and this group. [default: -1]
    --diagnostics   Run a diagnostic pre-test (MDMR) relating the non-neural scores to the nerual features.
    --debug         Verbose outputs.

DETAILS

The xbrain analysis uses a double-cross validation approach to demonstrate
whether the similarities in neural architecture or activity in the brain (X) is
predictive of non-neural variables of interest (Y). For example, demographics
(age, sex), cognitive scores (IQ, TASIT), or clinical variables (medications
taken, number of hospital visits in the last year).

** outer loop: stratified sample k fold cross validation**

The sample population is split int K distinct folds (e.g., --k=5, or 5 folds).
This requires a large sample (< 70 subjects preferred), but k-fold cross
validation is more robust to major outliers, which are likely in heterogenous
psychiatric populations. Briefly, K-fold cross validation splits the population
into a test set (n/K) and training set (the remainder). For each fold,
cross-brain correlations are calculated for each individual against the
template drawn from the training population only, so there is complete
independence between the test set and the template population. The training data
is used to train a classifier to predict the non-neural variable of interest
using these cross brain correlations. The performance of the model is evaluated
against the test set, and performance measures are averaged across all folds.

**inner loop: hyperparamater cross validation**

3 randomized fold cross validation is done to test all submitted hyperperameters
on splits of the training data. The best model is brought forward for testing.

The grid search for each model is defined in xbrain.stats.classify.

**features**

The primary feature used is cross-brain correlation of fMRI time series, but
this tool will also accept 3D stat maps (i.e., GLM scores, FA). These can be
combined because for each subject, we collapse the timeseries down to a vector
of cross brain correlation values the same length as the number of ROIs.

In order to do this, a mask with nonzero ROIs must be supplied. The number of
nonzero ROIs must be the same as the number of timeseries supplied. The average
statistic from within each ROI will be taken as the feature for that ROI.

During training and testing, the cross brain correlation values and statistic
vectors will be concatenated. One must be careful not to add too many
features into the model, as this can lead to overfitting and poor
generalizability.

xbrain -h or --help prints this message.
"""

import sys, os
import collections
import logging
import random
import string

import numpy as np
import scipy as sp
from scipy.stats import sem, mode
import pandas as pd
from sklearn.model_selection import StratifiedKFold

import xbrain.docopt as docopt
import xbrain.utils as utils
import xbrain.correlate as corr
import xbrain.stats as stats

logging.basicConfig(level=logging.WARN, format="[%(name)s] %(levelname)s: %(message)s")
logger = logging.getLogger(os.path.basename(__file__))

HOME = os.path.expanduser('~')

def assert_columns(db, columns):
    if not utils.is_column(db, columns):
        logger.error('not all columns {} found'.format(columns))
        sys.exit(1)

def clean(X):
    """Replaces nan and inf values in numpy array with zero"""
    X[np.isnan(X)] = 0
    X[np.isinf(X)] = 0
    logger.debug('X matrix has {} bad values (replaced with 0)'.format(sum(X == 0)))

    return(X)


def pre_test(db, timeseries, predict, n_features=None):
    """
    use MDMR to detect relationship between cognitive variables and MRI data.
    Cluster brain data, and show the mean cognitive score in each cluster. Good
    v scores are ~ 0.1, or 10%. If there are multiple predictors, uses PCA to
    reduce the feature space to 1D for plotting.
    """
    logger.info('pre-test: detecting gross relationship between neural and cognitive data')
    X = corr.calc_xbrain(db, db, timeseries)
    X = clean(X)
    if n_features:
        if X.shape[1] > n_features:
            X = stats.pca_reduce(X.T, n=n_features).T

    y = utils.gather_dv(db, predict)
    F, F_null, v = stats.mdmr(y.T, X, method='euclidean')
    thresholds = stats.sig_cutoffs(F_null, two_sided=False)

    if F > thresholds[1]:
        logger.info('pre-test: relationship detected: F={} > {}, variance explained={}'.format(F, thresholds[1], v))
    else:
        logger.warn('pre-test: no relationship detected, variance explained={}'.format(v))

    # reduce predictors to 1D for plotting
    if len(y.shape) == 2 and y.shape[0] > 1:
        y = stats.pca_reduce(y)

    np.savetxt(os.path.join(HOME, 'xbrain_X.csv'), X, delimiter=',')
    np.savetxt(os.path.join(HOME, 'xbrain_y.csv'), y, delimiter=',')

    # generate plots
    corr.plot_X(X, HOME, title='features')
    clst = stats.cluster2(X, plot=HOME)
    stats.distributions(y, os.path.join(HOME, 'xbrain_y_dist.pdf'), np.zeros(len(y)))
    sys.exit()

def main():
    #arguments = docopt(__doc__)
    #database      = arguments['<database>']
    #timeseries    = arguments['--timeseries']
    #connectivity  = arguments['--connectivity']
    #statmaps      = arguments['--statmaps']
    #predict       = arguments['--predict']
    #roi_mask      = arguments['--roi_mask']
    #k             = int(arguments['--k'])
    #n_features    = arguments['--n-features']
    #target_cutoff = float(arguments['--cutoff'])
    #target_group  = int(arguments['--target-group'])
    #two_template  = int(arguments['--two-template'])
    #diagnostics   = arguments['--diagnostics']
    #debug         = arguments['--debug']

    global HOME

    # for testing
    database = '/projects/jviviano/data/xbrain/assets/database_xbrain.csv'
    timeseries = 'ts_imi_resid,ts_obs_resid'
    statmaps = None
    connectivity = None
    #predict = 'Part1_TotalCorrect,Part2_TotalCorrect,Part3_TotalCorrect,scog_er40_cr_columnpcr_value,scog_er40_crt_columnqcrt_value'
    predict = 'Part1_TotalCorrect,Part2_TotalCorrect,Part3_TotalCorrect'
    #predict = 'Diagnosis'
    roi_mask = None
    k = 5
    n_features = None
    target_cutoff = 0.5
    target_group = 1
    two_template = 0
    diagnostics = True
    debug = True

    logger.info('starting')
    if debug:
        logger.setLevel(logging.DEBUG)

    logger.debug('checking inputs')
    try:
        db = pd.read_csv(database)
    except:
        logger.error('failed to parse input .csv database {}',format(database))
        sys.exit(1)

    # check and format inputs
    if not utils.is_probability(target_cutoff):
        logger.error('group target_cutoff percentile invalid for one group analysis: {} (0 < p < 1)'.format(target_cutoff))
        sys.exit(1)


    if two_template > -1:
        if two_template == target_group:
            logger.error('two_template group {} should be different than the target group {}'.format(two_template, target_group))
            sys.exit(1)

    if n_features:
        n_features = int(n_features)

    # check all defined database columns
    columns = []
    if statmaps:
        statmaps = statmaps.split(',')
        assert_columns(db, timeseries)
        columns.extend(statmaps)

    if timeseries:
        timeseries = timeseries.split(',')
        assert_columns(db, timeseries)
        columns.extend(timeseries)

    predict = predict.split(',')
    assert_columns(db, predict)
    columns.extend(predict)

    # reduce data to defined columns
    db = db[columns]
    n_pre = len(db)
    db = db.dropna(axis=0)
    logger.debug('columns in reduced database: {}'.format(columns))
    logger.debug('clean rows in database: {}/{}'.format(len(db), n_pre))

    if diagnostics:
        pre_test(db, timeseries, predict, n_features=n_features)

    logger.debug('generating dependent variable vector y')
    y = utils.gather_dv(db, predict)
    if len(y.shape) == 2 and y.shape[0] > 1:
        logger.info('using PCA to reduce {} dvs in y down to 1 component'.format(y.shape[0]))
        y = stats.pca_reduce(y)

    # split into groups if the data is continuous
    if len(np.unique(y)) > 10:
        logger.info('splitting y into two groups: {} percentile cutoff'.format(target_cutoff))
        y = utils.make_dv_groups(y, target_cutoff)

    y = stats.make_classes(y)

    # add a column with the preprocessed y groups
    y_uid = ''.join(random.choice(string.ascii_uppercase) for _ in range(10))
    db[y_uid] = y

    # ensure target_group, two_template group is one of the groups in y
    if target_group not in y:
        logger.error('target group {} not in y: {}'.format(target_group, np.unique(y)))
        sys.exit(1)

    if two_template > -1:
        if two_template not in y:
            logger.error('two template group {} not in y: {}'.format(two_template, np.unique(y)))
            sys.exit(1)

    # stores outputs across folds
    acc_train, acc_test, f1_train, f1_test, auc_train, auc_test = [], [], [], [], [], []
    hp_dict = collections.defaultdict(list)

    # stratified k-fold cross validation: split into train and test sets
    logger.info('Outer Loop: {} fold cross validation'.format(k))
    kf = StratifiedKFold(n_splits=k, shuffle=True)
    for train_idx, test_idx in kf.split(np.zeros(len(y)), y):

        # split the outcome variables into train and test groups
        y_train = y[train_idx]
        y_test = y[test_idx]

        # calculate X from a training sample template
        logger.debug('calculating template 1: group {}'.format(target_group))
        template_1 = corr.find_template(db.iloc[train_idx], y_uid, timeseries, group=target_group)
        X = corr.calc_xbrain(template_1, db, timeseries)

        if two_template > -1:
            logger.debug('calculating template 2: group {}'.format(two_template))
            template_2 = corr.find_template(db.iloc[train_idx], y_uid, timeseries, group=two_template)
            X2 = corr.calc_xbrain(template_2, db, timeseries)
            logger.debug('two template analysis: group {} - group {}'.format(target_group, two_template))
            X = X - X2

        # remove nonsense values from X
        X = clean(X)

        # if desired, compress the feature matrix X
        if n_features:
            if X.shape[1] > n_features:
                # double transpose required to reduce over features, not samples
                X = stats.pca_reduce(X.T, n=n_features).T

        # split X into test and train groups
        X_train = X[train_idx, :]
        X_test  = X[test_idx, :]

        corr.plot_X(X_train, HOME, title='test-vs-train', X2=X_test)

        # run the classifier
        try:
            results = stats.classify(X_train, X_test, y_train, y_test)
        except Exception as e:
            logger.error(e)
            sys.exit(1)

        # append the results of each fold
        acc_train.append(results['acc_train'])
        acc_test.append(results['acc_test'])
        f1_train.append(results['f1_train'])
        f1_test.append(results['f1_test'])
        auc_train.append(results['auc_train'])
        auc_test.append(results['auc_test'])

        # collects the relevant hyperparameters for the chosen model
        hp_tmp = results['hp_dict']
        for hp in hp_tmp.keys():
            hp_dict[hp].append(hp_tmp[hp])


    # find out most frequent hyperparameters during cross-val
    hp_mode = {}
    for hp in hp_dict.keys():
        hp_mode[hp] = mode(hp_dict[hp])[0][0]
    logger.debug('most frequent hp:\n  {}'.format(hp_mode))

    # final results
    print('train results:\n  acc={}+/-{}\n  f1={}+/-{}\n  auc={}+/-{}'.format(
             np.mean(acc_train), sem(acc_train), np.mean(f1_train), sem(f1_train), np.mean(auc_train), sem(auc_train)))
    print('test results:\n  acc={}+/-{}\n  f1={}+/-{}\n  auc={}+/-{}'.format(
             np.mean(acc_test), sem(acc_test), np.mean(f1_test), sem(f1_test), np.mean(auc_test), sem(auc_test)))


if __name__ == '__main__':
    main()

